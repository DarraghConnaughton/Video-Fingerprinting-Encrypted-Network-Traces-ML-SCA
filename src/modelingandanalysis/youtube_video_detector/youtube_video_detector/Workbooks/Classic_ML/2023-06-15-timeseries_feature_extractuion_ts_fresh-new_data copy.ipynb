{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TS Fresh to extract features on our dataset and classify using multiple logistic regression. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deborahdarragh/.local/lib/python3.10/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.metrics import roc_auc_score as AUC, accuracy_score as accuracy, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, cross_validate\n",
    "import sys\n",
    "sys.path.insert(0, '../.')\n",
    "from Utils import *\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../../../MCM1_Practicum_Data/\"\n",
    "meta_data_dir = data_dir+\"Metadata/\"\n",
    "binned_data_dir=data_dir+\"Pre-Processed_Data/data-ms-aggregates/\"\n",
    "lr_dir = data_dir+\"Pre-Processed_Data/LR/\"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ids_map_fn = meta_data_dir+\"all_ids_dict.json\"\n",
    "with open(video_ids_map_fn, \"r\") as json_file:\n",
    "    nums_to_vids_map = json.load(json_file)\n",
    "vids_to_nums_map = {v:k for k,v in nums_to_vids_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To model:  0\n"
     ]
    }
   ],
   "source": [
    "# Question, what is the best Binsize to predict a video using the DF?\n",
    "source_data_files =  os.listdir(binned_data_dir)#[0:5]\n",
    "\n",
    "# Get files for creating features\n",
    "feature_of_interest = \"DF\"\n",
    "feature_of_interest_folder = lr_dir+feature_of_interest+ \"/\"\n",
    "features_folder = feature_of_interest_folder +\"Features/\"\n",
    "train_folder = feature_of_interest_folder+\"Train/\"\n",
    "test_folder = feature_of_interest_folder+\"Test/\"\n",
    "models_folder = feature_of_interest_folder+\"Models/\"\n",
    "evaluation_folder = feature_of_interest_folder+\"Evaluation/\"\n",
    "\n",
    "validation_proportion = 0.3\n",
    "try:\n",
    "    os.mkdir(feature_of_interest_folder)\n",
    "    os.mkdir(features_folder)\n",
    "    os.mkdir(train_folder)\n",
    "    os.mkdir(test_folder)\n",
    "    os.mkdir(evaluation_folder)\n",
    "    os.mkdir(models_folder)\n",
    "except: pass\n",
    "\n",
    "already_modelled_bins = set([fn[5:9] for fn in os.listdir(evaluation_folder)])\n",
    "bins_to_model = set([fn[5:9] for fn in source_data_files]) - already_modelled_bins\n",
    "\n",
    "to_model_data_files = []\n",
    "for fn in source_data_files:\n",
    "    for bin_n in bins_to_model:\n",
    "        if str.__contains__(fn, bin_n):\n",
    "            to_model_data_files.append(fn)\n",
    "\n",
    "to_model_data_files = to_model_data_files\n",
    "fdr_level = 0.05\t\n",
    "print(\"To model: \",+len(to_model_data_files))\n",
    "\n",
    "\n",
    " # config\n",
    "# for test_fn in to_model_data_files:\n",
    "#     test_fp = binned_data_dir + test_fn\n",
    "#     features_file = features_folder+test_fn[:-8]+\"-\"+feature_of_interest+\"-features.csv\"\n",
    "#     train_file =  train_folder+test_fn[:-8]+\"-\"+feature_of_interest+\"-train.csv\"\n",
    "#     test_file =  test_folder+test_fn[:-8]+\"-\"+feature_of_interest+\"-test.csv\"\n",
    "#     df, target = transform_data(test_fp, feature_of_interest)\n",
    "#     extract_features_from_df(df, target, features_file)\n",
    "#     select_features_from_file(features_file, train_file, test_file, validation_proportion, fdr_level )\n",
    "#     train(train_fn, evaluation_folder):\n",
    "#     evaluate(model, test_file, evaluation_folder, vids_to_nums_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet(fp):\n",
    "    parquet_table = pq.read_table(fp)\n",
    "    return parquet_table.to_pandas()\n",
    "\n",
    "def transform_data(data:pd.DataFrame, feature_of_interest):\n",
    "    \n",
    "    cn = [feature_of_interest, \"video_id\"]\n",
    "    data = data[cn]\n",
    "    data[\"video_id\"] = data[\"video_id\"].map(vids_to_nums_map)\n",
    "    output_length = max(data[feature_of_interest].map(len))\n",
    "    data[feature_of_interest]= data[feature_of_interest].apply(lambda array: extend_list(list(array), max_length=output_length, filling_value=1)) \n",
    "    #shuffle\n",
    "    data = data.sample(len(data))\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    data[\"id\"] = data.index\n",
    "    data.rename({\"video_id\": \"target\", feature_of_interest:0}, axis=1, inplace=True)\n",
    "\n",
    "    y = data[\"target\"]\n",
    "    data.drop(\"target\", axis = 1, inplace = True )\n",
    "\n",
    "    # Explode the 'measurements' column\n",
    "    exploded_df = data.explode(0)\n",
    "\n",
    "    # Add the 'time' column\n",
    "    exploded_df['time'] = exploded_df.groupby('id').cumcount()\n",
    "\n",
    "    # Reset the index\n",
    "    exploded_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Reorder the columns\n",
    "    data = exploded_df[['id', 'time', 0]]\n",
    "    data[0] = data[0].map(float)\n",
    "\n",
    "    # Print the resulting dataframe\n",
    "    return(data, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_df(df, target, features_fn):\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter( \"ignore\" )\n",
    "        f = extract_features( df, column_id = \"id\", column_sort = \"time\")\n",
    "\n",
    "    impute( f )\n",
    "    assert f.isnull().sum().sum() == 0\n",
    "\n",
    "    f['y'] = target\n",
    "\n",
    "    f = f.sample(len(f))\n",
    "\n",
    "    f.to_csv( features_fn, index = None )\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    exicution_time = end_time  - start_time\n",
    "    m = int(exicution_time.seconds / 60)\n",
    "    s = exicution_time.seconds % 60\n",
    "    print(f\"Features extracted and saved to: \", features_fn)\n",
    "    print(f\"Duration: {m}:{s}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_from_file(features_fn, train_fn, test_fn, validation_proportion, fdr_level ):\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    print(\"loading {}\".format( features_fn ))\n",
    "    features = pd.read_csv( features_fn )\n",
    "\n",
    "    validation_split_i = int(len(features)*(1-validation_proportion))\n",
    "\n",
    "    train_x = features.iloc[:validation_split_i].drop( 'y', axis = 1 )\n",
    "    test_x = features.iloc[validation_split_i:].drop( 'y', axis = 1 )\n",
    "\n",
    "    train_y = features.iloc[:validation_split_i].y\n",
    "    test_y = features.iloc[validation_split_i:].y\n",
    "\n",
    "    print(\"selecting features...\")\n",
    "    train_features_selected = select_features( train_x, train_y, fdr_level = fdr_level )\n",
    "\n",
    "    print(\"selected {} features.\".format( len( train_features_selected.columns )))\n",
    "\n",
    "    train = train_features_selected.copy()\n",
    "    train['y'] = train_y\n",
    "\n",
    "    test = test_x[ train_features_selected.columns ].copy()\n",
    "    test['y'] = test_y\n",
    "\n",
    "\n",
    "    print(\"saving {}\".format( train_fn ))\n",
    "    train.to_csv( train_fn, index = None )\n",
    "\n",
    "    print(\"saving {}\".format( test_fn ))\n",
    "    test.to_csv( test_fn, index = None )\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    exicution_time = end_time  - start_time\n",
    "    m = int(exicution_time.seconds / 60)\n",
    "    s = exicution_time.seconds % 60\n",
    "\n",
    "    print(f\"Duration: {m}:{s}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training And Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_encoder(obj):\n",
    "    \"\"\"\n",
    "    Custom JSON encoder function to convert numeric keys represented as strings to actual numbers.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, str):\n",
    "        if obj.isnumeric():\n",
    "            return int(obj)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_fp, evaluation_folder):\n",
    "    start_time = datetime.now()\n",
    "    train = pd.read_csv( train_fp )\n",
    "    x_train = train.drop( 'y', axis = 1 ).values\n",
    "    y_train = train.y.values\n",
    "\n",
    "    # define the multinomial logistic regression model\n",
    "    model = LogisticRegression(multi_class='multinomial', solver='lbfgs',  max_iter=2000)\n",
    "    print(\"Model created.\")\n",
    "\n",
    "    # define the model evaluation procedure\n",
    "    scoring = ['precision_macro', 'recall_macro', 'accuracy', 'f1_macro']\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler().fit(x_train)\n",
    "    x_train_scaled = scaler.transform(x_train)\n",
    "    print(\"Begin validation\")\n",
    "    # evaluate the model and collect the scores\n",
    "    train_scores = cross_validate(model, x_train_scaled, y_train, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "\n",
    "    print(\"Validation complete\")\n",
    "    train_scores_dict = {str.replace(k, \"test_\", \"\"):{\n",
    "    \"mean\": np.mean(train_scores[k]),\n",
    "    \"std\": np.std(train_scores[k])\n",
    "    }\n",
    "        for k in train_scores.keys()\n",
    "    }\n",
    "    train_scores_dict\n",
    "\n",
    "    print(\"Fitting model\")\n",
    "    model.fit(x_train_scaled, y_train)\n",
    "    print(\"Fitting complete\")\n",
    "    train_fn = re.search(r\"/Train/(.*)\", train_file).group(1)\n",
    "    model_fn = models_folder+train_fn[:-10]+\"-lr-model.joblib\"\n",
    "    joblib.dump(model, model_fn)\n",
    "    print(\"Model saved as: \", model_fn)\n",
    "\n",
    "    evaluation_train_fn = evaluation_folder+train_fn[:-10]+\"-eval-train.json\"\n",
    "\n",
    "    with open(evaluation_train_fn, \"w\") as f: \n",
    "        json.dump(train_scores_dict, f, indent=4, sort_keys=True, default=key_encoder)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    exicution_time = end_time  - start_time\n",
    "    m = int(exicution_time.seconds / 60)\n",
    "    s = exicution_time.seconds % 60\n",
    "    print(f\"Training complete in: {m}:{s}\")\n",
    "    return model, pd.DataFrame(train_scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_fp, evaluation_folder, vids_to_nums_map): \n",
    "    test = pd.read_csv( test_fp )\n",
    "    x_test = test.drop( 'y', axis = 1 ).values\n",
    "    y_test = test.y.values\n",
    "    scaler = StandardScaler().fit(x_test)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    predictions = model.predict(x_test_scaled)\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    v_ids = list(set(vids_to_nums_map.values()) and set(y_test))\n",
    "    cm = confusion_matrix(y_test, predictions, labels = v_ids )\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    report[\"cm\"] = cm_normalized.tolist()\n",
    "    test_fn = re.search(r\"/Test/(.*)\", test_fp).group(1)\n",
    "    evaluation_test_fn = evaluation_folder+test_fn[:-9]+\"-eval-test.json\"\n",
    "    with open(evaluation_test_fn, \"w\") as f: \n",
    "        json.dump(report, f, indent=4, sort_keys=True, default=key_encoder)\n",
    "    del report[\"cm\"]\n",
    "    return pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_model_data_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m num_models \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(to_model_data_files)\n\u001b[1;32m      3\u001b[0m total_num_models \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(source_data_files)\n\u001b[1;32m      4\u001b[0m last_model_duration \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'to_model_data_files' is not defined"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "num_models = len(to_model_data_files)\n",
    "total_num_models = len(source_data_files)\n",
    "last_model_duration = \"\"\n",
    "for test_fn in [12]:\n",
    "    print(\"Last modeling duration: \"+ last_model_duration)\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    print(\"############################################################################\")\n",
    "    print(f\"                              Model {i+1+(total_num_models-num_models)}/{total_num_models} \")\n",
    "    print(\"############################################################################\")\n",
    "\n",
    "    test_fn = to_model_data_files[i]\n",
    "    test_fp = binned_data_dir + test_fn\n",
    "    features_file = features_folder+test_fn[:-8]+\"-\"+feature_of_interest+\"-features.csv\"\n",
    "    train_file =  train_folder+test_fn[:-8]+\"-\"+feature_of_interest+\"-train.csv\"\n",
    "    test_file =  test_folder+test_fn[:-8]+\"-\"+feature_of_interest+\"-test.csv\"\n",
    "    df, target = transform_data(test_fp, feature_of_interest)\n",
    "    extract_features_from_df(df, target, features_file)\n",
    "    select_features_from_file(features_file, train_file, test_file, validation_proportion, fdr_level )\n",
    "\n",
    "    model, evaluation_train = train(train_file, evaluation_folder)\n",
    "    display(evaluation_train)\n",
    "    del model\n",
    "    train_fn = re.search(r\"/Train/(.*)\", train_file).group(1)\n",
    "    model = loaded_model = joblib.load(models_folder+train_fn[:-10]+\"-lr-model.joblib\")\n",
    "    evaluation_test = evaluate(model, test_file, evaluation_folder, vids_to_nums_map)\n",
    "    display(evaluation_test)\n",
    "    i+=1\n",
    "    end_time = datetime.now()\n",
    "    exicution_time = end_time  - start_time\n",
    "    m = int(exicution_time.seconds / 60)\n",
    "    s = exicution_time.seconds % 60\n",
    "    last_model_duration = f\"{m}:{s}\"\n",
    "    clear_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v_ids = list(set(vids_to_nums_map.values()) and set(y_test))\n",
    "#cm = confusion_matrix(y_test, predictions, labels = v_ids )\n",
    "#cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#plt.figure(figsize=(10, 10)) \n",
    "#ax = sns.heatmap(cm_normalized, annot=False, cmap='Blues', xticklabels=v_ids, yticklabels=v_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predict probabilities with a multinomial logistic regression model\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# # define dataset\n",
    "# X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
    "# # define the multinomial logistic regression model\n",
    "# model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "# # fit the model on the whole dataset\n",
    "# model.fit(X, y)\n",
    "# # define a single row of input data\n",
    "# row = [1.89149379, -0.39847585, 1.63856893, 0.01647165, 1.51892395, -3.52651223, 1.80998823, 0.58810926, -0.02542177, -0.52835426]\n",
    "# # predict a multinomial probability distribution\n",
    "# yhat = model.predict_proba([row])\n",
    "# # summarize the predicted probabilities\n",
    "# print('Predicted Probabilities: %s' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the multinomial logistic regression model with a default penalty\n",
    "# LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the dataset\n",
    "# def get_dataset():\n",
    "# \tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3)\n",
    "# \treturn X, y\n",
    "\n",
    "# # get a list of models to evaluate\n",
    "# def get_models():\n",
    "# \tmodels = dict()\n",
    "# \tfor p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "# \t\t# create name for model\n",
    "# \t\tkey = '%.4f' % p\n",
    "# \t\t# turn off penalty in some cases\n",
    "# \t\tif p == 0.0:\n",
    "# \t\t\t# no penalty in this case\n",
    "# \t\t\tmodels[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
    "# \t\telse:\n",
    "# \t\t\tmodels[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
    "# \treturn models\n",
    "\n",
    "# # evaluate a give model using cross-validation\n",
    "# def evaluate_model(model, X, y):\n",
    "# \t# define the evaluation procedure\n",
    "# \tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# \t# evaluate the model\n",
    "# \tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# \treturn scores\n",
    "\n",
    "# # define dataset\n",
    "# X, y = get_dataset()\n",
    "# # get the models to evaluate\n",
    "# models = get_models()\n",
    "# # evaluate the models and store results\n",
    "# results, names = list(), list()\n",
    "# for name, model in models.items():\n",
    "# \t# evaluate the model and collect the scores\n",
    "# \tscores = evaluate_model(model, X, y)\n",
    "# \t# store the results\n",
    "# \tresults.append(scores)\n",
    "# \tnames.append(name)\n",
    "# \t# summarize progress along the way\n",
    "# \tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# # plot model performance for comparison\n",
    "# pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "# pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1099dae2b5f73178a2be407e5a1039f93f7b612f7a0579b6547d6a739f56e4a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
